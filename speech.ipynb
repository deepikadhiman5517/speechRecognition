{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepikadhiman5517/speechRecognition/blob/main/speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhHu70JKqn1Y"
      },
      "source": [
        "# Frame Level Speech Recognition with Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jlvKnM3qedT"
      },
      "source": [
        "In this coursework you will take your knowledge of feedforward neural networks and apply it to the task of speech recognition.\n",
        "\n",
        "You are provided a dataset of audio recordings (utterances) and their phoneme state (subphoneme) labels. The data comes from articles published in the Wall Street Journal (WSJ) that are read aloud and labelled using the original text. If you have not encountered speech data before or have not heard of phonemes or spectrograms, we will clarify these here:\n",
        "\n",
        "Phonemes and Phoneme States\n",
        "As letters are the atomic elements of written language, phonemes are the atomic elements of speech. It is crucial for us to have a means to distiguish different sounds in speech that may or may not represent the same letter or combinations of letters in the written alphabet. For example, the words \"jet\" and \"ridge\" both contain the same sound and we refer to this elemental sound as the phoneme \"JH\". For this challenge we will consider 46 phonemes in the english language.\n",
        "\n",
        "[\"+BREATH+\", \"+COUGH+\", \"+NOISE+\", \"+SMACK+\", \"+UH+\", \"+UM+\", \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\", \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"SIL\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
        "\n",
        "A powerful technique in speech recognition is to model speech as a markov process with unobserved states. This model considers observed speech to be dependent on unobserved state transitions. We refer to these unobserved states as phoneme states or subphonemes. For each phoneme, there are 3 respective phoneme states. Therefore for our 46 phonemes, there exist 138 respective phoneme states. The transition graph of the phoneme states for a given phoneme is as follows:\n",
        "\n",
        "\n",
        "Hidden Markov Models (HMMs) estimate the parameters of this unobserved markov process (transition and emission probabilities) that maximize the likelihood of the observed speech data.\n",
        "\n",
        "Your task is to instead take a model-free approach and classify mel spectrogram frames using a neural network that takes a frame (plus optional context) and outputs class probabilities for all 138 phoneme states. Performance on the task will be measured by classification accuracy on a held-out set of labelled mel spectrogram frames. Training/dev labels are provided as integers [0-137].\n",
        "\n",
        "Representing speech\n",
        "As a first step, the speech must be converted into a feature representation that can be fed into the network.\n",
        "\n",
        "In our representation, utterances have been converted to \"mel spectrograms\", which are pictorial representations that characterize how the frequency content of the signal varies with time. The frequency-domain of the audio signal provides more useful features for distinguishing phonemes.\n",
        "\n",
        "For a more intuitive understanding, consider attempting to determine which instruments are playing in an orchestra given an audio recording of a performance. By looking only at the amplitude of the signal of the orchestra over time, it is nearly impossible to distinguish one source from another. But if the signal is transformed into the frequency domain, we can use our knowledge that flutes produce higher frequency sounds and bassoons produce lower frequency sounds. In speech, a similar phenomenon is observed when the vocal tract produces sounds at varying frequencies.\n",
        "\n",
        "To convert the speech to a mel spectrogram, it is segmented into little \"frames\", each 25ms wide, where the \"stride\" between adjacent frames is 10ms. Thus we get 100 such frames per second of speech.\n",
        "\n",
        "From each frame, we compute a single \"mel spectral\" vector, where the components of the vector represent the (log) energy in the signal in different frequency bands. In the data we have given you, we have 40-dimensional mel-spectral vectors, i.e. we have computed energies in 40 frequency bands.\n",
        "\n",
        "Thus, we get 100 40-dimensional mel spectral (row) vectors per second of speech in the recording. Each one of these vectors is referred to as a frame. The details of how mel spectrograms are computed from speech is explained in the attached blog.\n",
        "\n",
        "Thus, for a T-second recording, the entire spectrogram is a 100T x 40 matrix, comprising 100T 40- dimensional vectors (at 100 vectors (frames) per second).\n",
        "\n",
        "The training data comprise:\n",
        "\n",
        "Speech recordings (raw mel spectrogram frames)\n",
        "Frame-level phoneme state labels\n",
        "The test data comprise:\n",
        "\n",
        "Speech recordings (raw mel spectrogram frames)\n",
        "Phoneme state labels are not given\n",
        "Your job is to identify the phoneme state label for each frame in the test data set. It is important to note that utterances are of variable length. We are providing you code to load and parse the raw files into the expected format. For now we are only providing dev data files as the training file is very large.\n",
        "\n",
        "Feature Files\n",
        "[train|dev|test].npy contain a numpy object array of shape [utterances]. Each utterance is a float32 ndarray of shape [time, frequency], where time is the length of the utterance. Frequency dimension is always 40 but time dimension is of variable length.\n",
        "\n",
        "Label Files\n",
        "[train|dev]_labels.npy contain a numpy object array of shape [utterances]. Each element in the array is an int32 array of shape [time] and provides the phoneme state label for each frame. There are 138 distinct labels [0-137], one for each subphoneme.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaooeurPqcZJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rf9BHKSq-a2"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "current_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-NV0tJOrN0B"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='speech_recognition')\n",
        "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
        "parser.add_argument('--batch_size', default=512, type=int, help='batch size')\n",
        "parser.add_argument('--context_size', default=12, type=int, help='context size')\n",
        "parser.add_argument('--input_size', default=1000, type=int, help='input size')\n",
        "parser.add_argument('--output_size', default=138, type=int, help='output size')\n",
        "parser.add_argument('--num_epochs', default=18, type=int, help='epoch number')\n",
        "parser.add_argument('--decay_steps', default='7, 12', type=str,\n",
        "                    help='The step where learning rate decay by 0.1')\n",
        "parser.add_argument('--save_step', default=5, type=int, help='step for saving model')\n",
        "parser.add_argument('--eval_step', default=1, type=int, help='step for validation')\n",
        "parser.add_argument('--train_data_path', default='/content/drive/MyDrive/dev.npy')\n",
        "parser.add_argument('--train_label_path', default='/content/drive/MyDrive/dev_labels.npy')\n",
        "parser.add_argument('--val_data_path', default='/content/drive/MyDrive/dev.npy', type=str)\n",
        "parser.add_argument('--val_label_path', default='/content/drive/MyDrive/dev_labels.npy', type=str)\n",
        "parser.add_argument('--test_data_path', default='../data/test.npy', type=str)\n",
        "parser.add_argument('--checkpoint_dir', default='../checkpoints/', help='checkpoint folder root')\n",
        "parser.add_argument('--result_file_name', default='hw1p2_test_result.csv', type=str, help='testing result save path')\n",
        "\n",
        "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV81UBImr6FR"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args()\n",
        "args.expr_dir = os.path.join(args.checkpoint_dir, current_time)\n",
        "os.makedirs(args.expr_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyq_ofTrt5yM"
      },
      "outputs": [],
      "source": [
        "# Create the log\n",
        "log_path = os.path.join(args.expr_dir, 'speech_recognition_{}.log'.format(current_time))\n",
        "logging.basicConfig(filename=log_path, level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXhPbPYpuIHZ"
      },
      "outputs": [],
      "source": [
        "# Modify the result save path\n",
        "args.result_file_name = os.path.join(args.expr_dir, args.result_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv0yZBx-uL8g"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxjWskSyuR45"
      },
      "outputs": [],
      "source": [
        "def save_log(message):\n",
        "    print(message)\n",
        "    logging.info(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zC_tFfbuZmI"
      },
      "outputs": [],
      "source": [
        "class load_dataset(Dataset):\n",
        "    def __init__(self, data_path, label_path=None):\n",
        "    \t# Both data and label has the same time length for one utterrance\n",
        "    \t# Data shape: (utterance, seq_len, 40), Label shape: (utterance, seq_len)\n",
        "        self.data = np.load(data_path, encoding='bytes', allow_pickle=True)\n",
        "        if label_path:\n",
        "            self.label = np.load(label_path,encoding='bytes',allow_pickle=True)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "        self.idx_map = []\n",
        "        for i, xs in enumerate(self.data):\n",
        "            for j in range(xs.shape[0]):\n",
        "                self.idx_map.append((i, j))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i, j = self.idx_map[index]\n",
        "        # Select the context_size before and after the current frame\n",
        "        x = self.data[i].take(range(j - args.context_size, j + args.context_size + 1), mode='clip', axis=0).flatten()\n",
        "        # Normalize\n",
        "        # x = (x - x.mean()) / x.std()\n",
        "        # Select the phoneme state label for the current frame\n",
        "        y = np.int32(self.label[i][j]).reshape(1) if self.label is not None else np.int32(-1).reshape(1)\n",
        "        return torch.from_numpy(x).float(), torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKX1r4dgupi3"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# * Layers -> [input_size, 2048, 2048, 1024, 1024, output_size]\n",
        "# * ReLU activations\n",
        "# * Context size k = 12 frames on both sides\n",
        "# * Adam optimizer, with the default learning rate 1e-3\n",
        "# * Zero padding of k frames on both sides of each utterance\n",
        "###\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(input_size, 2048),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(2048),\n",
        "                                 nn.Linear(2048, 2048),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(2048),\n",
        "                                 nn.Linear(2048, 2048),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(2048),\n",
        "                                 nn.Linear(2048, 1024),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(1024),\n",
        "                                 nn.Linear(1024, 1024),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(1024),\n",
        "                                 nn.Linear(1024, 1024),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(1024),\n",
        "                                 nn.Linear(1024, 1024),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(1024),\n",
        "                                 nn.Linear(1024, 512),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(512),\n",
        "                                 nn.Linear(512, 512),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(512),\n",
        "                                 nn.Linear(512, 512),\n",
        "                                 nn.ReLU(inplace=True),\n",
        "                                 nn.BatchNorm1d(512),\n",
        "                                 nn.Linear(512, output_size)\n",
        "                                 )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswX_dcjuq-F"
      },
      "outputs": [],
      "source": [
        "def train(net, loader, optimizer, criterion, epoch):\n",
        "    net.train()\n",
        "\n",
        "    running_batch = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over images.\n",
        "    for i, (data, label) in enumerate(loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        output = net(data)\n",
        "        _, label_pred = torch.max(output, 1)\n",
        "        loss = criterion(output, label.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_batch += label.size(0)\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(label_pred == label.view(-1)).item()\n",
        "\n",
        "        if (i + 1) % 20 == 0:  # print every 5 mini-batches\n",
        "            message = '[%d, %5d] loss: %.3f accuracy: %.3f' % (\n",
        "            epoch, i + 1, running_loss / running_batch, running_corrects / running_batch)\n",
        "            save_log(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB43v8owuyFj"
      },
      "outputs": [],
      "source": [
        "def validate(net, loader, criterion, epoch):\n",
        "    net.eval()\n",
        "\n",
        "    running_batch = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        message = '*' * 40\n",
        "        save_log(message)\n",
        "        for i, (data, label) in enumerate(loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = net(data)\n",
        "\n",
        "            # label_pred = torch.nn.functional.softmax(output, dim=1)\n",
        "            _, label_pred = torch.max(output, 1)\n",
        "\n",
        "            loss = criterion(output, label.view(-1))\n",
        "            running_batch += label.size(0)\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(label_pred == label.view(-1)).item()\n",
        "\n",
        "        running_loss /= running_batch\n",
        "        acc = running_corrects / running_batch\n",
        "        message = 'Epoch: %d, testing Loss %.3f, testing accuracy: %.3f' % (epoch, running_loss, acc)\n",
        "        save_log(message)\n",
        "        message = '*' * 40\n",
        "        save_log(message)\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-ejecbgu4fn"
      },
      "outputs": [],
      "source": [
        "def test(net, loader):\n",
        "    net.eval()\n",
        "    label = []\n",
        "    running_batch = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(loader):\n",
        "            data = data.to(device)\n",
        "            output = net(data)\n",
        "            _, label_pred = torch.max(output, 1)\n",
        "            label.extend(label_pred.cpu().numpy())\n",
        "            running_batch += data.size(0)\n",
        "    return running_batch, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXqph9f8vW6d"
      },
      "outputs": [],
      "source": [
        "def save_networks(net, which_epoch):\n",
        "    save_filename = '%s_net.pth' % (which_epoch)\n",
        "    save_path = os.path.join(args.expr_dir, save_filename)\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.save(net.module.cpu().state_dict(), save_path)\n",
        "        except:\n",
        "            torch.save(net.cpu().state_dict(), save_path)\n",
        "    else:\n",
        "        torch.save(net.cpu().state_dict(), save_path)\n",
        "\n",
        "\n",
        "def weights_init(m, type='kaiming'):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1 or classname.find('Conv2d') != -1:\n",
        "        if type == 'xavier':\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "        elif type == 'kaiming':\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        elif type == 'orthogonal':\n",
        "            nn.init.orthogonal_(m.weight)\n",
        "        elif type == 'gaussian':\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB3DAREAvc9X",
        "outputId": "5587a399-2377-48fb-ed55-e94fc730a62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging data\n",
            "Data is loaded\n",
            "epoch: 1/18 , lr: 0.0010\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,    20] loss: 0.011 accuracy: 0.094\n",
            "[1,    40] loss: 0.009 accuracy: 0.127\n",
            "[1,    60] loss: 0.008 accuracy: 0.151\n",
            "[1,    80] loss: 0.008 accuracy: 0.172\n",
            "[1,   100] loss: 0.007 accuracy: 0.187\n",
            "[1,   120] loss: 0.007 accuracy: 0.201\n",
            "[1,   140] loss: 0.007 accuracy: 0.214\n",
            "[1,   160] loss: 0.007 accuracy: 0.224\n",
            "[1,   180] loss: 0.007 accuracy: 0.234\n",
            "[1,   200] loss: 0.006 accuracy: 0.244\n",
            "[1,   220] loss: 0.006 accuracy: 0.252\n",
            "[1,   240] loss: 0.006 accuracy: 0.260\n",
            "[1,   260] loss: 0.006 accuracy: 0.268\n",
            "[1,   280] loss: 0.006 accuracy: 0.274\n",
            "[1,   300] loss: 0.006 accuracy: 0.281\n",
            "[1,   320] loss: 0.006 accuracy: 0.288\n",
            "[1,   340] loss: 0.006 accuracy: 0.294\n",
            "[1,   360] loss: 0.006 accuracy: 0.300\n",
            "[1,   380] loss: 0.006 accuracy: 0.306\n",
            "[1,   400] loss: 0.006 accuracy: 0.311\n",
            "[1,   420] loss: 0.005 accuracy: 0.315\n",
            "[1,   440] loss: 0.005 accuracy: 0.320\n",
            "[1,   460] loss: 0.005 accuracy: 0.324\n",
            "[1,   480] loss: 0.005 accuracy: 0.329\n",
            "[1,   500] loss: 0.005 accuracy: 0.333\n",
            "[1,   520] loss: 0.005 accuracy: 0.336\n",
            "[1,   540] loss: 0.005 accuracy: 0.339\n",
            "[1,   560] loss: 0.005 accuracy: 0.343\n",
            "[1,   580] loss: 0.005 accuracy: 0.346\n",
            "[1,   600] loss: 0.005 accuracy: 0.349\n",
            "[1,   620] loss: 0.005 accuracy: 0.352\n",
            "[1,   640] loss: 0.005 accuracy: 0.355\n",
            "[1,   660] loss: 0.005 accuracy: 0.357\n",
            "[1,   680] loss: 0.005 accuracy: 0.360\n",
            "[1,   700] loss: 0.005 accuracy: 0.363\n",
            "[1,   720] loss: 0.005 accuracy: 0.365\n",
            "[1,   740] loss: 0.005 accuracy: 0.368\n",
            "[1,   760] loss: 0.005 accuracy: 0.370\n",
            "[1,   780] loss: 0.005 accuracy: 0.372\n",
            "[1,   800] loss: 0.005 accuracy: 0.374\n",
            "[1,   820] loss: 0.005 accuracy: 0.376\n",
            "[1,   840] loss: 0.005 accuracy: 0.379\n",
            "[1,   860] loss: 0.005 accuracy: 0.381\n",
            "[1,   880] loss: 0.005 accuracy: 0.383\n",
            "[1,   900] loss: 0.005 accuracy: 0.384\n",
            "[1,   920] loss: 0.005 accuracy: 0.386\n",
            "[1,   940] loss: 0.005 accuracy: 0.388\n",
            "[1,   960] loss: 0.005 accuracy: 0.390\n",
            "[1,   980] loss: 0.005 accuracy: 0.392\n",
            "[1,  1000] loss: 0.005 accuracy: 0.393\n",
            "[1,  1020] loss: 0.005 accuracy: 0.395\n",
            "[1,  1040] loss: 0.005 accuracy: 0.396\n",
            "[1,  1060] loss: 0.005 accuracy: 0.398\n",
            "[1,  1080] loss: 0.005 accuracy: 0.400\n",
            "[1,  1100] loss: 0.005 accuracy: 0.401\n",
            "[1,  1120] loss: 0.005 accuracy: 0.402\n",
            "[1,  1140] loss: 0.005 accuracy: 0.404\n",
            "[1,  1160] loss: 0.005 accuracy: 0.405\n",
            "[1,  1180] loss: 0.005 accuracy: 0.407\n",
            "[1,  1200] loss: 0.005 accuracy: 0.408\n",
            "[1,  1220] loss: 0.004 accuracy: 0.409\n",
            "[1,  1240] loss: 0.004 accuracy: 0.411\n",
            "[1,  1260] loss: 0.004 accuracy: 0.412\n",
            "[1,  1280] loss: 0.004 accuracy: 0.413\n",
            "[1,  1300] loss: 0.004 accuracy: 0.414\n",
            "****************************************\n",
            "Epoch: 1, testing Loss 0.004, testing accuracy: 0.499\n",
            "****************************************\n",
            "epoch: 2/18 , lr: 0.0010\n",
            "----------\n",
            "[2,    20] loss: 0.004 accuracy: 0.496\n",
            "[2,    40] loss: 0.004 accuracy: 0.498\n",
            "[2,    60] loss: 0.004 accuracy: 0.503\n",
            "[2,    80] loss: 0.004 accuracy: 0.503\n",
            "[2,   100] loss: 0.004 accuracy: 0.503\n",
            "[2,   120] loss: 0.004 accuracy: 0.504\n",
            "[2,   140] loss: 0.004 accuracy: 0.506\n",
            "[2,   160] loss: 0.004 accuracy: 0.507\n",
            "[2,   180] loss: 0.004 accuracy: 0.507\n",
            "[2,   200] loss: 0.004 accuracy: 0.507\n",
            "[2,   220] loss: 0.004 accuracy: 0.507\n",
            "[2,   240] loss: 0.004 accuracy: 0.508\n",
            "[2,   260] loss: 0.004 accuracy: 0.508\n",
            "[2,   280] loss: 0.004 accuracy: 0.509\n",
            "[2,   300] loss: 0.004 accuracy: 0.509\n",
            "[2,   320] loss: 0.004 accuracy: 0.509\n",
            "[2,   340] loss: 0.004 accuracy: 0.509\n",
            "[2,   360] loss: 0.004 accuracy: 0.510\n",
            "[2,   380] loss: 0.004 accuracy: 0.510\n",
            "[2,   400] loss: 0.004 accuracy: 0.510\n",
            "[2,   420] loss: 0.004 accuracy: 0.511\n",
            "[2,   440] loss: 0.004 accuracy: 0.512\n",
            "[2,   460] loss: 0.004 accuracy: 0.512\n",
            "[2,   480] loss: 0.004 accuracy: 0.512\n",
            "[2,   500] loss: 0.004 accuracy: 0.512\n",
            "[2,   520] loss: 0.004 accuracy: 0.512\n",
            "[2,   540] loss: 0.004 accuracy: 0.513\n",
            "[2,   560] loss: 0.004 accuracy: 0.514\n",
            "[2,   580] loss: 0.004 accuracy: 0.514\n",
            "[2,   600] loss: 0.004 accuracy: 0.514\n",
            "[2,   620] loss: 0.004 accuracy: 0.515\n",
            "[2,   640] loss: 0.004 accuracy: 0.515\n",
            "[2,   660] loss: 0.004 accuracy: 0.515\n",
            "[2,   680] loss: 0.004 accuracy: 0.516\n",
            "[2,   700] loss: 0.003 accuracy: 0.516\n",
            "[2,   720] loss: 0.003 accuracy: 0.516\n",
            "[2,   740] loss: 0.003 accuracy: 0.517\n",
            "[2,   760] loss: 0.003 accuracy: 0.517\n",
            "[2,   780] loss: 0.003 accuracy: 0.517\n",
            "[2,   800] loss: 0.003 accuracy: 0.517\n",
            "[2,   820] loss: 0.003 accuracy: 0.518\n",
            "[2,   840] loss: 0.003 accuracy: 0.518\n",
            "[2,   860] loss: 0.003 accuracy: 0.518\n",
            "[2,   880] loss: 0.003 accuracy: 0.518\n",
            "[2,   900] loss: 0.003 accuracy: 0.519\n",
            "[2,   920] loss: 0.003 accuracy: 0.519\n",
            "[2,   940] loss: 0.003 accuracy: 0.519\n",
            "[2,   960] loss: 0.003 accuracy: 0.519\n",
            "[2,   980] loss: 0.003 accuracy: 0.520\n",
            "[2,  1000] loss: 0.003 accuracy: 0.520\n",
            "[2,  1020] loss: 0.003 accuracy: 0.521\n",
            "[2,  1040] loss: 0.003 accuracy: 0.521\n",
            "[2,  1060] loss: 0.003 accuracy: 0.521\n",
            "[2,  1080] loss: 0.003 accuracy: 0.522\n",
            "[2,  1100] loss: 0.003 accuracy: 0.522\n",
            "[2,  1120] loss: 0.003 accuracy: 0.522\n",
            "[2,  1140] loss: 0.003 accuracy: 0.522\n",
            "[2,  1160] loss: 0.003 accuracy: 0.522\n",
            "[2,  1180] loss: 0.003 accuracy: 0.523\n",
            "[2,  1200] loss: 0.003 accuracy: 0.523\n",
            "[2,  1220] loss: 0.003 accuracy: 0.523\n",
            "[2,  1240] loss: 0.003 accuracy: 0.523\n",
            "[2,  1260] loss: 0.003 accuracy: 0.524\n",
            "[2,  1280] loss: 0.003 accuracy: 0.524\n",
            "[2,  1300] loss: 0.003 accuracy: 0.524\n",
            "****************************************\n",
            "Epoch: 2, testing Loss 0.003, testing accuracy: 0.553\n",
            "****************************************\n",
            "epoch: 3/18 , lr: 0.0010\n",
            "----------\n",
            "[3,    20] loss: 0.003 accuracy: 0.551\n",
            "[3,    40] loss: 0.003 accuracy: 0.554\n",
            "[3,    60] loss: 0.003 accuracy: 0.557\n",
            "[3,    80] loss: 0.003 accuracy: 0.558\n",
            "[3,   100] loss: 0.003 accuracy: 0.556\n",
            "[3,   120] loss: 0.003 accuracy: 0.555\n",
            "[3,   140] loss: 0.003 accuracy: 0.555\n",
            "[3,   160] loss: 0.003 accuracy: 0.555\n",
            "[3,   180] loss: 0.003 accuracy: 0.554\n",
            "[3,   200] loss: 0.003 accuracy: 0.554\n",
            "[3,   220] loss: 0.003 accuracy: 0.555\n",
            "[3,   240] loss: 0.003 accuracy: 0.556\n",
            "[3,   260] loss: 0.003 accuracy: 0.556\n",
            "[3,   280] loss: 0.003 accuracy: 0.555\n",
            "[3,   300] loss: 0.003 accuracy: 0.555\n",
            "[3,   320] loss: 0.003 accuracy: 0.554\n",
            "[3,   340] loss: 0.003 accuracy: 0.554\n",
            "[3,   360] loss: 0.003 accuracy: 0.555\n",
            "[3,   380] loss: 0.003 accuracy: 0.555\n",
            "[3,   400] loss: 0.003 accuracy: 0.555\n",
            "[3,   420] loss: 0.003 accuracy: 0.556\n",
            "[3,   440] loss: 0.003 accuracy: 0.556\n",
            "[3,   460] loss: 0.003 accuracy: 0.556\n",
            "[3,   480] loss: 0.003 accuracy: 0.557\n",
            "[3,   500] loss: 0.003 accuracy: 0.557\n",
            "[3,   520] loss: 0.003 accuracy: 0.557\n",
            "[3,   540] loss: 0.003 accuracy: 0.557\n",
            "[3,   560] loss: 0.003 accuracy: 0.557\n",
            "[3,   580] loss: 0.003 accuracy: 0.557\n",
            "[3,   600] loss: 0.003 accuracy: 0.558\n",
            "[3,   620] loss: 0.003 accuracy: 0.558\n",
            "[3,   640] loss: 0.003 accuracy: 0.558\n",
            "[3,   660] loss: 0.003 accuracy: 0.558\n",
            "[3,   680] loss: 0.003 accuracy: 0.558\n",
            "[3,   700] loss: 0.003 accuracy: 0.558\n",
            "[3,   720] loss: 0.003 accuracy: 0.559\n",
            "[3,   740] loss: 0.003 accuracy: 0.559\n",
            "[3,   760] loss: 0.003 accuracy: 0.559\n",
            "[3,   780] loss: 0.003 accuracy: 0.559\n",
            "[3,   800] loss: 0.003 accuracy: 0.559\n",
            "[3,   820] loss: 0.003 accuracy: 0.559\n",
            "[3,   840] loss: 0.003 accuracy: 0.559\n",
            "[3,   860] loss: 0.003 accuracy: 0.559\n",
            "[3,   880] loss: 0.003 accuracy: 0.560\n",
            "[3,   900] loss: 0.003 accuracy: 0.560\n",
            "[3,   920] loss: 0.003 accuracy: 0.560\n",
            "[3,   940] loss: 0.003 accuracy: 0.560\n",
            "[3,   960] loss: 0.003 accuracy: 0.561\n",
            "[3,   980] loss: 0.003 accuracy: 0.561\n",
            "[3,  1000] loss: 0.003 accuracy: 0.561\n",
            "[3,  1020] loss: 0.003 accuracy: 0.561\n",
            "[3,  1040] loss: 0.003 accuracy: 0.561\n",
            "[3,  1060] loss: 0.003 accuracy: 0.561\n",
            "[3,  1080] loss: 0.003 accuracy: 0.561\n",
            "[3,  1100] loss: 0.003 accuracy: 0.562\n",
            "[3,  1120] loss: 0.003 accuracy: 0.562\n",
            "[3,  1140] loss: 0.003 accuracy: 0.562\n",
            "[3,  1160] loss: 0.003 accuracy: 0.562\n",
            "[3,  1180] loss: 0.003 accuracy: 0.562\n",
            "[3,  1200] loss: 0.003 accuracy: 0.562\n",
            "[3,  1220] loss: 0.003 accuracy: 0.562\n",
            "[3,  1240] loss: 0.003 accuracy: 0.562\n",
            "[3,  1260] loss: 0.003 accuracy: 0.562\n",
            "[3,  1280] loss: 0.003 accuracy: 0.563\n",
            "[3,  1300] loss: 0.003 accuracy: 0.563\n",
            "****************************************\n",
            "Epoch: 3, testing Loss 0.003, testing accuracy: 0.587\n",
            "****************************************\n",
            "epoch: 4/18 , lr: 0.0010\n",
            "----------\n",
            "[4,    20] loss: 0.003 accuracy: 0.591\n",
            "[4,    40] loss: 0.003 accuracy: 0.588\n",
            "[4,    60] loss: 0.003 accuracy: 0.592\n",
            "[4,    80] loss: 0.003 accuracy: 0.591\n",
            "[4,   100] loss: 0.003 accuracy: 0.592\n",
            "[4,   120] loss: 0.003 accuracy: 0.591\n",
            "[4,   140] loss: 0.003 accuracy: 0.591\n",
            "[4,   160] loss: 0.003 accuracy: 0.591\n",
            "[4,   180] loss: 0.003 accuracy: 0.590\n",
            "[4,   200] loss: 0.003 accuracy: 0.591\n",
            "[4,   220] loss: 0.003 accuracy: 0.591\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    net = MLP(input_size=args.input_size, output_size=args.output_size)\n",
        "    net.apply(weights_init)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion.to(device)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
        "\n",
        "    str_steps = args.decay_steps.split(',')\n",
        "    args.decay_steps = []\n",
        "    for str_step in str_steps:\n",
        "        str_step = int(str_step)\n",
        "        args.decay_steps.append(str_step)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=args.decay_steps, gamma=0.1)\n",
        "\n",
        "    save_log('Logging data')\n",
        "    train_data = load_dataset(args.train_data_path, args.train_label_path)\n",
        "    train_loader = DataLoader(dataset=train_data, num_workers=4, batch_size=args.batch_size, pin_memory=True,\n",
        "                              shuffle=True)\n",
        "    val_data = load_dataset(args.val_data_path, args.val_label_path)\n",
        "    val_loader = DataLoader(dataset=val_data, num_workers=4, batch_size=args.batch_size, pin_memory=True,\n",
        "                            shuffle=False)\n",
        "    save_log('Data is loaded')\n",
        "    cur_acc = 0\n",
        "    for epoch in range(1, args.num_epochs + 1):\n",
        "        net.to(device)\n",
        "\n",
        "        scheduler.step()\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        message = '{}: {}/{} , {}: {:.4f}'.format('epoch', epoch, args.num_epochs, 'lr', lr)\n",
        "        save_log(message)\n",
        "        save_log('-' * 10)\n",
        "\n",
        "        train(net, train_loader, optimizer, criterion, epoch)\n",
        "        if epoch % args.eval_step == 0:\n",
        "            val_acc = validate(net, val_loader, criterion, epoch)\n",
        "\n",
        "            if val_acc > cur_acc:\n",
        "                save_networks(net, epoch)\n",
        "                cur_acc = val_acc\n",
        "\n",
        "        # if epoch % args.save_step == 0:\n",
        "        #     save_networks(epoch)\n",
        "    save_networks(net, epoch)\n",
        "\n",
        "    # ------------------------\n",
        "    # Start Testing\n",
        "    # ------------------------\n",
        "    save_log('Loading test data')\n",
        "    test_data = load_dataset(args.test_data_path)\n",
        "    test_loader = DataLoader(dataset=test_data, num_workers=4, batch_size=args.batch_size, pin_memory=True, shuffle=False)\n",
        "    save_log('Test data is loaded')\n",
        "    net.to(device)\n",
        "    test_num, test_label = test(net, test_loader)\n",
        "    d = {'id': list(range(test_num)), 'label': test_label}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    df.to_csv(args.file_name, header=True, index=False)\n",
        "    save_log('Testing is done, result is saved to {}'.format(args.result_file_name))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yoYT0mMD887Tu2pkEIGViayvuGLoRQDM",
      "authorship_tag": "ABX9TyMxs75S6qE1atKo82Li+bkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}